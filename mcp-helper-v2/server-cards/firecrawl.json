{
  "id": "firecrawl",
  "name": "Firecrawl MCP Server",
  "status": "active",
  "transports": ["stdio"],
  "runtime": "node",
  "deploy": {
    "kind": "npx",
    "command": "npx",
    "args": ["-y", "firecrawl-mcp"]
  },
  "envSchema": [
    {
      "name": "FIRECRAWL_API_KEY",
      "description": "Firecrawl API key for web scraping and extraction",
      "required": true,
      "example": "fc-..."
    }
  ],
  "ports": [],
  "healthCheck": {
    "command": "echo 'Firecrawl MCP server configured'"
  },
  "docs": [
    {
      "title": "Firecrawl MCP Server",
      "url": "https://github.com/firecrawl-ai/firecrawl-mcp"
    },
    {
      "title": "Firecrawl Documentation",
      "url": "https://docs.firecrawl.dev"
    }
  ],
  "useCases": {
    "generic": [
      "Web page scraping and content extraction",
      "Structured data extraction from websites",
      "Website crawling and mapping",
      "Converting web content to markdown"
    ],
    "project": [
      "Scrape MCP server documentation websites",
      "Extract structured information for server cards",
      "Map MCP server documentation sites",
      "Convert web documentation to markdown format"
    ]
  },
  "prosCons": {
    "pros": [
      "Powerful web scraping capabilities",
      "Handles JavaScript-rendered content",
      "Structured data extraction with LLM",
      "Supports batch operations and crawling"
    ],
    "cons": [
      "Requires API key with usage limits",
      "Costs associated with API usage",
      "Rate limits based on plan"
    ]
  },
  "agenticUsefulness": {
    "humanVerificationRating": 2,
    "aiAgentRating": 4,
    "ratingRationale": {
      "human": "Low - Primarily an automation tool. Humans verify scraped content quality but don't interact directly with the scraping process.",
      "agent": "Very High - Essential for web research and documentation extraction. Enables AI to access and understand web content that would otherwise be inaccessible."
    },
    "bestPractices": [
      "Cache scraped content to avoid repeated API calls",
      "Use structured extraction for consistent data formats",
      "Batch related scraping operations together",
      "Prefer map before crawl to understand site structure"
    ],
    "humanRole": "Define scraping requirements, verify extracted data quality, manage API costs",
    "aiRole": "Execute scraping operations, extract structured data, analyze web content",
    "integrationSynergies": [
      "Perplexity for finding sources → Firecrawl for detailed extraction",
      "Firecrawl for documentation → Memory for persistent storage",
      "Context7 for official docs → Firecrawl for implementation examples"
    ]
  }
}