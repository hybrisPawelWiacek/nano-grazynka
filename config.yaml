server:
  port: 3101
  host: 0.0.0.0

database:
  url: file:/data/nano-grazynka.db

transcription:
  provider: openai  # Using OpenAI for gpt-4o-transcribe model
  model: gpt-4o-transcribe  # GPT-4o-Transcribe: 30% better accuracy than whisper-1 at same cost
  defaultModel: gpt-4o-transcribe  # Default model selection
  maxFileSizeMB: 25
  supportedFormats:
    - mp3
    - mp4
    - mpeg
    - mpga
    - m4a
    - wav
    - webm
  
  # Multi-model configuration
  models:
    gpt-4o-transcribe:
      provider: openai
      endpoint: /v1/audio/transcriptions
      maxPromptTokens: 224
      responseFormat: json
      costPerMinute: 0.006
      processingTime: "5-10 seconds"
      
    google/gemini-2.0-flash-001:
      provider: gemini-direct  # Using direct Google Gemini API
      endpoint: /v1beta/models/gemini-2.0-flash-exp:generateContent
      apiKey: GEMINI_API_KEY  # Uses environment variable
      maxPromptTokens: 1000000
      supportsTemplates: true
      requiresBase64: true
      costPerMinute: 0.0015
      processingTime: "10-20 seconds"

summarization:
  provider: openrouter  # Primary provider (fallback to openai)
  model: google/gemini-2.5-flash  # Latest Gemini Flash model via OpenRouter
  # Alternative models:
  # - google/gemini-2.0-flash-001 (previous version)
  # - google/gemini-2.5-pro-exp-03-25 (higher reasoning)
  # - gpt-4o-mini (OpenAI fallback)
  maxTokens: 2000
  temperature: 0.7
  # Prompts moved to backend/prompts.yaml for better maintainability

titleGeneration:
  provider: openrouter  # Use same provider as summarization for consistency
  model: google/gemini-2.5-flash  # Fast and efficient for metadata extraction
  maxTokens: 150  # Small output for title and brief description
  temperature: 0.3  # Lower temperature for more consistent results
  # Prompt moved to backend/prompts.yaml

storage:
  uploadDir: /data/uploads
  maxFileAgeDays: 30

processing:
  maxConcurrentJobs: 3
  jobTimeoutMinutes: 30
  retryAttempts: 3
  queueType: memory
  statusUpdateIntervalMs: 5000